{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#NLP Assignment 4\n",
        "##Name := Yash Bhaskar Narkhede"
      ],
      "metadata": {
        "id": "e9Cf62jC-nLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN?\n",
        "\n",
        "Ans:\n",
        "\n",
        "Sequence-to-Sequence RNN:\n",
        "\n",
        "Machine Translation\n",
        "\n",
        "Image Captioning\n",
        "\n",
        "Text Summarization\n",
        "\n",
        "Sequence-to-Vector RNN:\n",
        "\n",
        "****\n",
        "Speech Recognition\n",
        "\n",
        "Document Classification\n",
        "\n",
        "Sentiment Analysis\n",
        "\n",
        "Vector-to-Sequence RNN:\n",
        "****\n",
        "Music Generation\n",
        "\n",
        "Text Generation\n",
        "\n",
        "Generative Art\n",
        "\n",
        "#2. Why do people use encoder–decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
        "\n",
        "Ans: Encoder–decoder RNNs are better suited for automatic translation than plain sequence-to-sequence RNNs because they are able to better capture the context of a sentence. An encoder–decoder RNN uses two separate recurrent neural networks, one to encode the source sentence into a fixed-length vector and the other to decode the fixed-length vector into a target sentence. This allows for more accurate translations because the context of the source sentence is better preserved.\n",
        "\n",
        "#3. How could you combine a convolutional neural network with an RNN to classify videos?\n",
        "\n",
        "Ans: A convolutional neural network combined with an RNN can be used to classify videos by first using the convolutional neural network to extract features from each frame of the video and then using the RNN to analyze the extracted features in order to classify the video as belonging to a certain class. The RNN will be able to look at the extracted features over a period of time and detect patterns that would not be visible if only looking at a single frame. The resulting classification should be more accurate than using either technique alone.\n",
        "\n",
        "#4. What are the advantages of building an RNN using dynamic_rnn() rather than static_rnn()?\n",
        "\n",
        "Ans: 1. Dynamic_rnn() is more computationally efficient than static_rnn() because it allows for variable-length inputs and does not require the user to specify the input sequence length prior to model training.\n",
        "\n",
        "Dynamic_rnn() allows for greater flexibility in constructing the model, as it allows for the creation of more complex recurrent architectures such as bidirectional RNNs and stacked RNNs.\n",
        "\n",
        "The dynamic_rnn() function also allows for efficient backpropagation through time, allowing for faster training and better performance on time series data.\n",
        "\n",
        "#5. How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
        "\n",
        "Ans: For variable-length input sequences, one can use padding or truncation. Padding involves adding an appropriate value (e.g. zeros) to the beginning or end of shorter sequences, so that all inputs have the same length. Truncation involves discarding information from the end of longer sequences, so that all inputs have the same length.\n",
        "\n",
        "For variable-length output sequences, one can use a technique called bucketing. Bucketing involves grouping sequences of similar lengths together, and then training a separate model for each group. This allows the model to learn different patterns for each group and thus produce variable-length output sequences.\n",
        "\n",
        "#6. What is a common way to distribute training and execution of a deep RNN across multiple GPUs?\n",
        "\n",
        "Ans: One common way to distribute training and execution of a deep RNN across multiple GPUs is to use a data-parallel approach. This involves splitting the data across different GPUs, and then training the network on each GPU in parallel. The results from all GPUs are then aggregated together and used to update the model parameters. This approach allows for much faster training and execution of deep RNNs, since the workload is distributed across multiple GPUs.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "lBZtYp80-zGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wI-XAQuh_QW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ne84fee-mQC"
      },
      "outputs": [],
      "source": []
    }
  ]
}